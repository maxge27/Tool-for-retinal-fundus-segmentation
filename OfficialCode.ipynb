{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-V1bjcPtrIr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install opencv-python-headless"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*-- -- -- -- -- -- CARICAMENTO IMMAGINI -- -- -- -- -- --*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n_KnshGcTBda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "4pHpUgUyCMMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import random\n",
        "\n",
        "def load_data(path, seed=42):\n",
        "    random.seed(seed)  # Imposta un seed per la riproducibilità\n",
        "\n",
        "    # Carica i percorsi dei file\n",
        "    train_x = sorted(glob(os.path.join(path, \"training\", \"train_x\", \"*.png\")))\n",
        "    train_y = sorted(glob(os.path.join(path, \"training\", \"train_y\", \"*.png\")))\n",
        "\n",
        "    val_x = sorted(glob(os.path.join(path, \"validation\", \"validation_x\", \"*.png\")))\n",
        "    val_y = sorted(glob(os.path.join(path, \"validation\", \"validation_y\", \"*.png\")))\n",
        "\n",
        "    test_x = sorted(glob(os.path.join(path, \"test\", \"test_x\", \"*.png\")))\n",
        "    test_y = sorted(glob(os.path.join(path, \"test\", \"test_y\", \"*.png\")))\n",
        "\n",
        "\n",
        "    return (train_x, train_y), (val_x, val_y), (test_x, test_y)\n"
      ],
      "metadata": {
        "id": "MBSM9hObYw9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    \"\"\" Load the data \"\"\"\n",
        "    data_path = \"/content/drive/MyDrive/Progetto/RETINA\"\n",
        "    (train_x, train_y), (val_x, val_y), (test_x, test_y) = load_data(data_path)\n"
      ],
      "metadata": {
        "id": "3XKHf6XyCg_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*-- -- -- -- -- -- CREAZIONE DELLA CLASSE RetinaDataset -- -- -- -- -- --*\n"
      ],
      "metadata": {
        "id": "vnBs_yfQT2q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import os\n",
        "\n",
        "class RetinaDataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_paths, resize_to=(512, 512), patch_size=256, augment=False, use_patches=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.resize_to = resize_to\n",
        "        self.patch_size = patch_size\n",
        "        self.augment = augment\n",
        "        self.use_patches = use_patches\n",
        "        self.transforms = self.get_transforms(augment)\n",
        "\n",
        "    def __len__(self):\n",
        "        if not self.use_patches:\n",
        "            return len(self.image_paths)\n",
        "        else:\n",
        "            # Calcola il numero di patches per immagine considerando l'overlap\n",
        "            step_size = self.patch_size - 128\n",
        "            num_patches_x = (self.resize_to[0] - 128) // step_size\n",
        "            num_patches_y = (self.resize_to[1] - 128) // step_size\n",
        "            num_patches_per_image = num_patches_x * num_patches_y\n",
        "            return len(self.image_paths) * num_patches_per_image\n",
        "\n",
        "    def get_patches(self, image, mask, img_name):\n",
        "          image = image.resize(self.resize_to)\n",
        "          mask = mask.resize(self.resize_to)\n",
        "\n",
        "          patches = []\n",
        "          mask_patches = []\n",
        "          patch_info = []\n",
        "\n",
        "          step_size = self.patch_size - 128  # Calcolo dello step considerando l'overlap\n",
        "          num_patches_x = (self.resize_to[0] - 128) // step_size\n",
        "          num_patches_y = (self.resize_to[1] - 128) // step_size\n",
        "\n",
        "          for i in range(num_patches_y):\n",
        "              for j in range(num_patches_x):\n",
        "                  x = j * step_size\n",
        "                    y = i * step_size\n",
        "                  patch = image.crop((x, y, x + self.patch_size, y + self.patch_size))\n",
        "                  mask_patch = mask.crop((x, y, x + self.patch_size, y + self.patch_size))\n",
        "                  patches.append(patch)\n",
        "                  mask_patches.append(mask_patch)\n",
        "                  patch_info.append(f\"{img_name}_patch_{x}_{y}\")\n",
        "\n",
        "          return patches, mask_patches, patch_info\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.use_patches:\n",
        "            # Calcola il numero di patches per dimensione\n",
        "            step_size = self.patch_size - 128\n",
        "            num_patches_x = (self.resize_to[0] - 128) // step_size\n",
        "            num_patches_y = (self.resize_to[1] - 128) // step_size\n",
        "            num_patches_per_image = num_patches_x * num_patches_y\n",
        "            img_idx = idx // num_patches_per_image\n",
        "            patch_idx = idx % num_patches_per_image\n",
        "\n",
        "            image_path = self.image_paths[img_idx]\n",
        "            mask_path = self.mask_paths[img_idx]\n",
        "            img_name = os.path.splitext(os.path.basename(image_path))[0]  # Estrae il nome del file senza estensione\n",
        "\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            r, g, b = image.split()  # Divide l'immagine nei suoi canali RGB\n",
        "            image = g  # Utilizza solo il canale verde\n",
        "            mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "            patches, mask_patches, patch_info = self.get_patches(image, mask, img_name)\n",
        "            image, mask = patches[patch_idx], mask_patches[patch_idx]\n",
        "            patch_name = patch_info[patch_idx]  # Recupera il nome della patch corrente\n",
        "\n",
        "            if self.transforms:\n",
        "                sample = self.transforms(image=np.array(image), mask=np.array(mask))\n",
        "                image, mask = sample['image'], sample['mask']\n",
        "\n",
        "        else:\n",
        "            image_path = self.image_paths[idx]\n",
        "            mask_path = self.mask_paths[idx]\n",
        "            img_name = os.path.splitext(os.path.basename(image_path))[0]  # Estrae il nome del file senza estensione\n",
        "\n",
        "            image = Image.open(image_path).convert('L')\n",
        "            mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "            if self.transforms:\n",
        "                sample = self.transforms(image=np.array(image), mask=np.array(mask))\n",
        "                image, mask = sample['image'], sample['mask']\n",
        "            patch_name = img_name  # Usa il nome dell'immagine originale se non si usano le patches\n",
        "\n",
        "        image = image.clone().detach().to(dtype=torch.float32) / 255.0\n",
        "        mask = mask.clone().detach().to(dtype=torch.float32) / 255.0\n",
        "\n",
        "        return image, mask, patch_name\n",
        "\n",
        "    def get_transforms(self, augment):\n",
        "        if augment:\n",
        "            return A.Compose([\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.VerticalFlip(p=0.5),\n",
        "                A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=0.2),\n",
        "                A.Rotate(limit=45, p=0.5),\n",
        "\n",
        "                ToTensorV2()\n",
        "\n",
        "\n",
        "            ])\n",
        "        else:\n",
        "            return A.Compose([\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "\n"
      ],
      "metadata": {
        "id": "V1V1tPSrVTnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "train_dataset = RetinaDataset(train_x, train_y, augment=True, use_patches=True)\n",
        "val_dataset = RetinaDataset(val_x, val_y, augment=False, use_patches=True)\n",
        "test_dataset = RetinaDataset(test_x, test_y, augment=False, use_patches=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "# Assicurati che la tupla restituita contenga 3 elementi (immagine, maschera, nome della patch) prima di eseguire questo\n",
        "image_sample, mask_sample, patch_name = train_dataset[0]  # Prendiamo la prima immagine nel dataset di addestramento\n",
        "print(\"Tipo dell'immagine:\", type(image_sample))\n",
        "print(\"Forma dell'immagine:\", image_sample.shape)\n",
        "print(\"Nome della patch:\", patch_name)\n",
        "\n",
        "#Calcoliamo il numero di patch per immagine con l'overlap di 128 pixel\n",
        "num_patches_per_image = 9  # 9 patch per immagine, dato l'overlap di 128 pixel\n",
        "\n",
        "# Iteriamo sulle prime 3 immagini e le loro patch\n",
        "for img_idx in range(3):  # Per le prime 3 immagini\n",
        "    print(f\"Immagini {img_idx + 1}:\")\n",
        "    for patch_idx in range(num_patches_per_image):\n",
        "        # Calcola l'indice globale della patch nel dataset\n",
        "        global_patch_idx = img_idx * num_patches_per_image + patch_idx\n",
        "        _, _, patch_name = train_dataset[global_patch_idx]  # Otteniamo la patch dal dataset\n",
        "        print(f\"Patch {patch_idx + 1}: {patch_name}\")\n",
        "    print(\"-\" * 20)  # Separa le informazioni delle immagini per una migliore leggibilità\n"
      ],
      "metadata": {
        "id": "BtVg9RzzWhRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Otteniamo l'immagine e la maschera originali per la prima immagine del validation set senza dividerle in patches\n",
        "val_dataset = RetinaDataset(val_x, val_y, augment=False, use_patches=False)  # use_patches impostato su False per ottenere l'immagine intera\n",
        "image_sample, mask_sample, _ = val_dataset[0]  # Ignoriamo il nome della patch\n",
        "\n",
        "# Convertiamo i campioni da tensori PyTorch a array numpy e rimuoviamo le dimensioni aggiuntive\n",
        "image_sample_np = image_sample.numpy().squeeze()\n",
        "mask_sample_np = mask_sample.numpy().squeeze()\n",
        "\n",
        "# Plot dell'immagine originale con la maschera sovrapposta\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(image_sample_np, cmap='gray')  # Immagine in scala di grigi\n",
        "plt.imshow(mask_sample_np, alpha=0.5, cmap='jet')  # Sovrapposizione della maschera con opacità al 50%\n",
        "plt.title(\"Immagine Originale con Maschera Sovrapposta\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Ora, ripristiniamo il dataset per utilizzare le patches e plotteremo le patches con le rispettive maschere\n",
        "val_dataset = RetinaDataset(val_x, val_y, augment=False, use_patches=True)  # use_patches impostato su True per le patches\n",
        "patches = []\n",
        "mask_patches = []\n",
        "for patch_idx in range(num_patches_per_image):\n",
        "    patch_image, patch_mask, _ = val_dataset[patch_idx]\n",
        "    patches.append(patch_image.numpy().squeeze())\n",
        "    mask_patches.append(patch_mask.numpy().squeeze())\n",
        "\n",
        "# Plot di tutte le patches con le rispettive maschere sovrapposte\n",
        "fig, axs = plt.subplots(3, 3, figsize=(10, 10))  # Griglia 3x3 per le patches\n",
        "for idx in range(9):\n",
        "    ax = axs[idx // 3, idx % 3]\n",
        "    ax.imshow(patches[idx], cmap='gray')  # Patch in scala di grigi\n",
        "    ax.imshow(mask_patches[idx], alpha=0.1, cmap='jet')  # Sovrapposizione della maschera con opacità al 50%\n",
        "    ax.set_title(f\"Patch {idx+1}\")\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vb4OzrklD-pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " -- -- -- -- -- UNET -- -- -- -- --"
      ],
      "metadata": {
        "id": "dk24zAosfzb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from zipfile import ZipFile\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "PCaBkaYdPEdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()  # Funzione o per assicurare la riproducibilità\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n",
        "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n",
        "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512))\n",
        "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.conv1 = DoubleConv(512, 256)\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.conv2 = DoubleConv(256, 128)\n",
        "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.conv3 = DoubleConv(128, 64)\n",
        "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x = self.up1(x4)\n",
        "        x = self.conv1(torch.cat([x, x3], dim=1))\n",
        "        x = self.up2(x)\n",
        "        x = self.conv2(torch.cat([x, x2], dim=1))\n",
        "        x = self.up3(x)\n",
        "        x = self.conv3(torch.cat([x, x1], dim=1))\n",
        "        logits = self.outc(x)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "hhI2lfM8P8Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inizializza la rete U-Net\n",
        "model = UNet(n_channels=1, n_classes=1)\n"
      ],
      "metadata": {
        "id": "zU8mWbzvPlcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfmizKhlx60_"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Define Dice Loss. Since it's a loss function, each operation must be differentiable so we can backpropagate the gradients. To do this we use torch native functions.\n",
        "def dice_loss(outputs, labels, eps=1e-7):\n",
        "    intersection = torch.sum(outputs * labels)\n",
        "    union = torch.sum(outputs) + torch.sum(labels)\n",
        "    dice_score = 2.0 * intersection / (union + eps)\n",
        "    return 1.0 - dice_score\n",
        "\n",
        "criterion = dice_loss\n",
        "\n",
        "# Define optimizer with weight decay\n",
        "optimizer = Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYWO9CpVx60_"
      },
      "outputs": [],
      "source": [
        "def IOU(outputs, labels):\n",
        "    \"\"\"\n",
        "    Compute IOU for both PyTorch tensors and NumPy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy if necessary\n",
        "    if isinstance(outputs, torch.Tensor):\n",
        "        outputs = outputs.detach().cpu().numpy()\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.detach().cpu().numpy()\n",
        "\n",
        "    # Check if the output and labels are of the same size\n",
        "    if outputs.shape != labels.shape:\n",
        "        raise ValueError(f\"Expected output size ({outputs.shape}) to be same as target size ({labels.shape})\")\n",
        "\n",
        "    # Convert labels to boolean array\n",
        "    labels = labels.astype(bool)\n",
        "\n",
        "    # Threshold outputs to obtain binary values\n",
        "    outputs = outputs > 0.5\n",
        "\n",
        "    # Compute intersection and union\n",
        "    intersection = np.logical_and(labels, outputs)\n",
        "    union = np.logical_or(labels, outputs)\n",
        "\n",
        "    # Compute IOU\n",
        "    iou = np.sum(intersection) / np.sum(union)\n",
        "\n",
        "    return iou\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Numero di batch nei DataLoader\n",
        "num_batches_train = len(train_loader)\n",
        "num_batches_val = len(val_loader)\n",
        "num_batches_test = len(test_loader)\n",
        "\n",
        "print(f\"Numero di batch in Train Loader: {num_batches_train}\")\n",
        "print(f\"Numero di batch in Validation Loader: {num_batches_val}\")\n",
        "print(f\"Numero di batch in Test Loader: {num_batches_test}\")\n",
        "\n",
        "# Numero totale di elementi (assumendo ogni batch tranne l'ultimo sia pieno)\n",
        "total_items_train = len(train_loader.dataset)\n",
        "total_items_val = len(val_loader.dataset)\n",
        "total_items_test = len(test_loader.dataset)\n",
        "print(f\"Numero totale di elementi in Train Loader: {total_items_train}\")\n",
        "print(f\"Numero totale di elementi in Validation Loader: {total_items_val}\")\n",
        "print(f\"Numero totale di elementi in Test Loader: {total_items_test}\")"
      ],
      "metadata": {
        "id": "BWiTrOPawoZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-- -- -- -- -- TRAINING -- -- -- -- --**"
      ],
      "metadata": {
        "id": "ifrsT9hIgA2m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGWIdfG-x61A"
      },
      "outputs": [],
      "source": [
        "# Training settings\n",
        "n_epochs = 50\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "working_folder= \"/content/drive/MyDrive/Progetto/RETINA\"\n",
        "\n",
        "# Move model to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Start training loop\n",
        "\n",
        "early_stopping_patience = 7\n",
        "epochs_since_improvement = 0\n",
        "best_val_loss = float('inf')\n",
        "best_iou_score = 0.0\n",
        "\n",
        "# define two lists to store the training and validation losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "\n",
        "    # Initialize running loss to 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Add a progress bar for the training loop. This is done applying the function tqdm to the train_loader object\n",
        "    train_progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\")\n",
        "\n",
        "    for images, masks, patch_names in train_progress_bar:\n",
        "\n",
        "        # Move images and masks to device\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images).sigmoid().squeeze()\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update running loss\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Update progress bar\n",
        "        train_progress_bar.set_postfix(loss=running_loss / ((train_progress_bar.n + 1) * train_loader.batch_size))\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # store the training loss in the list\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize running loss and IOU score to 0\n",
        "    running_loss = 0.0\n",
        "    running_iou_score = 0.0\n",
        "\n",
        "    # Add a progress bar for the validation loop\n",
        "    val_progress_bar = tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks, patch_names in val_progress_bar:\n",
        "\n",
        "            # Move images and masks to device, compute outputs and loss\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = torch.sigmoid(model(images)) # sigmoid to convert the network output to probabilities between 0 and 1, the squeeze function removes the extra dimension\n",
        "            outputs = outputs.squeeze()\n",
        "            masks = masks.squeeze()\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            # Update running loss and IOU score\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_iou_score += IOU(outputs, masks).item() * images.size(0)\n",
        "\n",
        "            val_progress_bar.set_postfix(loss=running_loss / ((val_progress_bar.n + 1) * val_loader.batch_size))\n",
        "\n",
        "    # Compute validation loss and IOU score\n",
        "    val_loss = running_loss / len(val_loader.dataset)\n",
        "    iou_score = running_iou_score / len(val_loader.dataset)\n",
        "\n",
        "    # store the validation loss in the list\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "     # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        print(f'Modello migliorato da {best_val_loss:.4f} a {val_loss:.4f}. Salvataggio in corso...')\n",
        "        best_val_loss = val_loss\n",
        "        best_iou_score = iou_score\n",
        "        epochs_since_improvement = 0\n",
        "\n",
        "        # Salva il modello\n",
        "        model_dir = os.path.join(working_folder,'models')\n",
        "        if not os.path.exists(model_dir):\n",
        "            os.makedirs(model_dir)\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir,'model256over.pth'))\n",
        "        print(f'Modello salvato: epoca {epoch+1} con loss di validazione {best_val_loss:.4f} e IOU di validazione {best_iou_score:.4f}')\n",
        "    else:\n",
        "        epochs_since_improvement += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if epochs_since_improvement >= early_stopping_patience:\n",
        "        print(f'Interruzione anticipata attivata dopo {epoch + 1} epoche.')\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "        # After all epochs, plot and save the training and validation losses\n",
        "    loss_dir = os.path.join(working_folder, 'losses')\n",
        "    if not os.path.exists(loss_dir):\n",
        "        os.makedirs(loss_dir)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Val Loss')\n",
        "    plt.title('Training/Validation Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(loss_dir, \"final_loss_plot256over.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save model if validation loss has decreased in a folder called \"models\"\n",
        "    if val_loss < best_val_loss:\n",
        "\n",
        "        # update the best validation loss and IOU score\n",
        "        best_val_loss = val_loss\n",
        "        best_iou_score = iou_score\n",
        "\n",
        "        # create the folder if it does not exist\n",
        "        model_dir = os.path.join(working_folder,'models')\n",
        "        if not os.path.exists(model_dir):\n",
        "            os.makedirs(model_dir)\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir,'model256over.pth'))\n",
        "        print(f'Model saved at epoch {epoch+1} with validation loss of {best_val_loss:.4f} and validation IOU of {best_iou_score:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation IOU: {iou_score:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize your model here\n",
        "model = UNet(n_channels=1, n_classes=1).to(device)\n",
        "\n",
        "# Path to the folder where your model is saved\n",
        "working_folder = '/content/drive/MyDrive/Progetto/RETINA'\n",
        "model_dir = os.path.join(working_folder, 'models')\n",
        "\n",
        "# Load the model weights\n",
        "model_path = os.path.join(model_dir, 'model256over.pth')\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "\n",
        "def test_model(test_loader, model, criterion, device, save_folder, subset, save_predictions=False):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_iou_score = 0.0\n",
        "\n",
        "    if save_predictions:\n",
        "        # Crea la cartella per salvare le previsioni, se non esiste\n",
        "        save_folder = os.path.join(save_folder, subset)\n",
        "        if not os.path.exists(save_folder):\n",
        "            os.makedirs(save_folder)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, masks, patch_names) in enumerate(test_loader):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = torch.sigmoid(model(images))  # Converti l'output in probabilità\n",
        "            outputs = outputs.squeeze()\n",
        "            loss = criterion(outputs, masks)\n",
        "            iou = IOU(outputs, masks.squeeze())\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_iou_score += iou.item() * images.size(0)\n",
        "\n",
        "            if save_predictions:\n",
        "                # Converti gli output in maschere binarie e salva\n",
        "                outputs = outputs > 0.5\n",
        "                np_outputs = outputs.detach().cpu().numpy()\n",
        "\n",
        "                for i, output in enumerate(np_outputs):\n",
        "                    # Converti l'array in un'immagine PIL\n",
        "                    pil_out = Image.fromarray((output * 255).astype(np.uint8))\n",
        "                    # Salva l'immagine con un nome unico per ogni previsione, utilizzando le informazioni di patch_names\n",
        "                    patch_name = patch_names[i]\n",
        "                    pil_out.save(os.path.join(save_folder, f\"{patch_name}.png\"))\n",
        "\n",
        "    epoch_loss = running_loss / len(test_loader.dataset)\n",
        "    epoch_iou_score = running_iou_score / len(test_loader.dataset)\n",
        "\n",
        "    return epoch_loss, epoch_iou_score\n",
        "\n",
        "\n",
        "\n",
        "working_folder = '/content/drive/MyDrive/Progetto/RETINA'\n",
        "# create the folder if it does not exist\n",
        "DL_masks_dir = os.path.join(working_folder,'DL_masks256over')\n",
        "\n",
        "if not os.path.exists(DL_masks_dir):\n",
        "    os.makedirs(DL_masks_dir)\n",
        "\n",
        "# Load the best model\n",
        "model_dir = os.path.join(working_folder,'models')\n",
        "model.load_state_dict(torch.load(os.path.join(model_dir,'model256over.pth')))\n",
        "#model.load_state_dict(torch.load(os.path.join(model_dir,'model.pth'), map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "train_dataset = RetinaDataset(train_x, train_y, augment=False, use_patches=True)\n",
        "val_dataset = RetinaDataset(val_x, val_y, augment=False, use_patches=True)\n",
        "test_dataset = RetinaDataset(test_x, test_y, augment=False, use_patches=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Evaluate the model on the subsets\n",
        "train_loss, train_iou_score = test_model(train_loader, model, criterion, device, DL_masks_dir, 'train', save_predictions=True)\n",
        "print(f'Train Loss: {train_loss:.4f}, Train IOU: {train_iou_score:.4f}')\n",
        "\n",
        "val_loss, val_iou_score = test_model(val_loader, model, criterion, device, DL_masks_dir,  'val', save_predictions=True)\n",
        "print(f'Val Loss: {val_loss:.4f}, Val IOU: {val_iou_score:.4f}')\n",
        "\n",
        "test_loss, test_iou_score = test_model(test_loader, model, criterion, device, DL_masks_dir, 'test', save_predictions=True)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test IOU: {test_iou_score:.4f}')"
      ],
      "metadata": {
        "id": "6SvAMSnDwW9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_segmentation(test_loader, model, device, num_images=3):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Aggiunta di _ per catturare valori aggiuntivi restituiti dal DataLoader\n",
        "        for i, (images, masks, _) in enumerate(test_loader):\n",
        "            if i >= num_images:\n",
        "                break\n",
        "\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = torch.sigmoid(model(images))\n",
        "            outputs = outputs.squeeze()\n",
        "\n",
        "            for j in range(images.size(0)):\n",
        "                if i * images.size(0) + j >= num_images:\n",
        "                    break\n",
        "\n",
        "                plt.figure(figsize=(10, 4))\n",
        "\n",
        "                plt.subplot(1, 3, 1)\n",
        "                plt.imshow(images[j, 0].cpu().squeeze(), cmap='gray')\n",
        "                plt.title('Original Image')\n",
        "\n",
        "                plt.subplot(1, 3, 2)\n",
        "                plt.imshow(masks[j].cpu().squeeze(), cmap='gray')\n",
        "                plt.title('Ground Truth')\n",
        "\n",
        "                plt.subplot(1, 3, 3)\n",
        "                plt.imshow(outputs[j].cpu().squeeze(), cmap='gray')\n",
        "                plt.title('Predicted Segmentation')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "# Visualize segmentation results\n",
        "visualize_segmentation(test_loader, model, device)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3-izbatUfir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " -- -- -- -- -- RICOSTRUZIONE PATCHES -- -- -- -- --"
      ],
      "metadata": {
        "id": "hF9tTA0hgOPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def reconstruct_with_overlap(patch_files, patch_size, image_size, overlap, patches_directory, save_directory):\n",
        "    if not os.path.exists(save_directory):\n",
        "        os.makedirs(save_directory)\n",
        "\n",
        "    step_size = patch_size - overlap  # Calcola lo step size basandosi sull'overlap\n",
        "    num_patches_x = (image_size - overlap) // step_size\n",
        "    num_patches_y = (image_size - overlap) // step_size\n",
        "    patches_per_image = num_patches_x * num_patches_y\n",
        "\n",
        "    for i in range(0, len(patch_files), patches_per_image):\n",
        "        image_patches = patch_files[i:i+patches_per_image]\n",
        "        image_name = \"_\".join(image_patches[0].split('_')[:-3]) + '.png'\n",
        "        reconstructed_image = np.zeros((image_size, image_size), dtype=np.float32)\n",
        "        count_matrix = np.zeros((image_size, image_size), dtype=np.float32) + 1e-6  # Evita la divisione per zero\n",
        "\n",
        "        for patch_file in image_patches:\n",
        "            patch_path = os.path.join(patches_directory, patch_file)\n",
        "            patch = np.array(Image.open(patch_path), dtype=np.float32)\n",
        "            _, x_str, y_str = patch_file.split('_')[-3:]\n",
        "            x, y = int(x_str), int(y_str.split('.')[0])\n",
        "\n",
        "            reconstructed_image[y:y+patch_size, x:x+patch_size] += patch\n",
        "            count_matrix[y:y+patch_size, x:x+patch_size] += 1\n",
        "\n",
        "        # Fai la media dei valori nei punti di overlap\n",
        "        reconstructed_image /= count_matrix\n",
        "\n",
        "        # Converti l'array numpy in un'immagine PIL e salva\n",
        "        reconstructed_image_pil = Image.fromarray(np.uint8(reconstructed_image))\n",
        "        save_path = os.path.join(save_directory, image_name)\n",
        "        reconstructed_image_pil.save(save_path)\n",
        "\n",
        "# Parametri\n",
        "base_directory = '/content/drive/MyDrive/Progetto/RETINA/DL_masks256over'\n",
        "save_base_directory = '/content/drive/MyDrive/Progetto/RETINA/reconstructed512'\n",
        "patch_size = 256\n",
        "image_size = 512\n",
        "overlap = 128\n",
        "\n",
        "# Esegue la ricostruzione per ogni subset\n",
        "for subset in ['train', 'val', 'test']:\n",
        "    patches_directory = os.path.join(base_directory, subset)\n",
        "    save_directory = os.path.join(save_base_directory, subset)\n",
        "    patch_files = sorted([file for file in os.listdir(patches_directory) if file.endswith('.png')])\n",
        "\n",
        "    reconstruct_with_overlap(patch_files, patch_size, image_size, overlap, patches_directory, save_directory)\n"
      ],
      "metadata": {
        "id": "VJAHi8aBaE-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- -- -- -- -- CALCOLO DELLE METRICHE -- -- -- -- --"
      ],
      "metadata": {
        "id": "nvVXw1i2gVpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(mask_gt, reconstructed_mask):\n",
        "    intersection = np.logical_and(mask_gt, reconstructed_mask)\n",
        "    union = np.logical_or(mask_gt, reconstructed_mask)\n",
        "    iou = np.sum(intersection) / np.sum(union)\n",
        "    accuracy = np.sum(mask_gt == reconstructed_mask) / mask_gt.size\n",
        "    precision = np.sum(intersection) / np.sum(reconstructed_mask)\n",
        "    sensitivity = np.sum(intersection) / np.sum(mask_gt)\n",
        "    dice = 2 * np.sum(intersection) / (np.sum(mask_gt) + np.sum(reconstructed_mask))\n",
        "    return iou, accuracy, precision, sensitivity, dice"
      ],
      "metadata": {
        "id": "Ut_MbjrVl56A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from skimage.io import imread\n",
        "import os\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray, rgba2rgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "base_path = '/content/drive/MyDrive/Progetto/RETINA'\n",
        "\n",
        "# Percorsi alle cartelle con le maschere ground truth e le maschere ricostruite\n",
        "gt_masks_folders = {\n",
        "    'train': os.path.join(base_path, 'training', 'train_y'),\n",
        "    'val': os.path.join(base_path, 'validation', 'validation_y'),\n",
        "    'test': os.path.join(base_path, 'test', 'test_y')\n",
        "}\n",
        "reconstructed_masks_folders = {\n",
        "    'train': os.path.join(base_path, 'reconstructed512', 'train'),\n",
        "    'val': os.path.join(base_path, 'reconstructed512', 'val'),\n",
        "    'test': os.path.join(base_path, 'reconstructed512', 'test')\n",
        "}\n",
        "\n",
        "\n",
        "# Dizionario per memorizzare le metriche\n",
        "metrics = {phase: {'iou': [], 'accuracy': [], 'precision': [], 'sensitivity': [], 'dice': []}\n",
        "           for phase in ['train', 'val', 'test']}\n",
        "\n",
        "# Contatore per gli elementi non presi\n",
        "missing_elements_count = {'train': 0, 'val': 0, 'test': 0}\n",
        "\n",
        "def prepare_image(path):\n",
        "    \"\"\"\n",
        "    Carica un'immagine, la converte in scala di grigi se necessario, e la ridimensiona.\n",
        "    \"\"\"\n",
        "    image = imread(path)\n",
        "    if image.shape[-1] == 4:\n",
        "        image = rgba2rgb(image)\n",
        "    if len(image.shape) > 2:\n",
        "        image = rgb2gray(image)\n",
        "    image_resized = resize(image, (512, 512), anti_aliasing=True)\n",
        "    image_bin = image_resized > 0.5\n",
        "    return image_bin\n",
        "\n",
        "for phase in ['train', 'val', 'test']:\n",
        "    gt_masks_folder = gt_masks_folders[phase]\n",
        "    reconstructed_masks_folder = reconstructed_masks_folders[phase]\n",
        "\n",
        "    reconstructed_filenames_set = {os.path.splitext(fname.replace('_patch', ''))[0] for fname in os.listdir(reconstructed_masks_folder) if fname.endswith('.png')}\n",
        "\n",
        "    for fname in os.listdir(gt_masks_folder):\n",
        "        if not fname.endswith('.png') or fname == \"448_G.png\":\n",
        "            continue\n",
        "\n",
        "        base_fname = os.path.splitext(fname)[0]\n",
        "\n",
        "        if base_fname in reconstructed_filenames_set:\n",
        "            ground_truth_path = os.path.join(gt_masks_folder, fname)\n",
        "            reconstructed_mask_path = os.path.join(reconstructed_masks_folder, base_fname + '.png')\n",
        "\n",
        "            mask_gt = prepare_image(ground_truth_path)\n",
        "            reconstructed_mask = prepare_image(reconstructed_mask_path)\n",
        "\n",
        "            iou, accuracy, precision, sensitivity, dice = calculate_metrics(mask_gt, reconstructed_mask)\n",
        "\n",
        "            metrics[phase]['iou'].append(iou)\n",
        "            metrics[phase]['accuracy'].append(accuracy)\n",
        "            metrics[phase]['precision'].append(precision)\n",
        "            metrics[phase]['sensitivity'].append(sensitivity)\n",
        "            metrics[phase]['dice'].append(dice)\n",
        "        else:\n",
        "            missing_elements_count[phase] += 1\n",
        "\n",
        "\n",
        "def print_metrics(metrics):\n",
        "    for phase in ['train', 'val', 'test']:\n",
        "        print(f\"Metrics for {phase.upper()} phase:\")\n",
        "        for metric_name, metric_values in metrics[phase].items():\n",
        "            if metric_values:  # Verifica che la lista non sia vuota\n",
        "                mean_value = np.mean(metric_values)\n",
        "                std_value = np.std(metric_values)\n",
        "                print(f\"{metric_name.capitalize()}: mean = {mean_value:.4f} +/- {std_value:.4f}\")\n",
        "            else:\n",
        "                print(f\"{metric_name.capitalize()}: No data\")\n",
        "        print()  # Aggiunge una riga vuota per separare le fasi\n",
        "\n",
        "# Utilizza questa funzione modificata per stampare le metriche con media e deviazione standard\n",
        "print_metrics(metrics)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5EjS4rSXd8O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray, rgba2rgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "base_path = '/content/drive/MyDrive/Progetto/RETINA'\n",
        "\n",
        "# Percorsi alle cartelle con le maschere ground truth e le maschere ricostruite\n",
        "gt_masks_folders = {\n",
        "    'train': os.path.join(base_path, 'training', 'train_y'),\n",
        "    'val': os.path.join(base_path, 'validation', 'validation_y'),\n",
        "    'test': os.path.join(base_path, 'test', 'test_y')\n",
        "}\n",
        "reconstructed_masks_folders = {\n",
        "    'train': os.path.join(base_path, 'reconstructed512', 'train'),\n",
        "    'val': os.path.join(base_path, 'reconstructed512', 'val'),\n",
        "    'test': os.path.join(base_path, 'reconstructed512', 'test')\n",
        "}\n",
        "\n",
        "# Dizionario per memorizzare le metriche aggregate\n",
        "aggregated_metrics = {'iou': [], 'accuracy': [], 'precision': [], 'sensitivity': [], 'dice': []}\n",
        "\n",
        "def prepare_image(path):\n",
        "    \"\"\"\n",
        "    Carica un'immagine, la converte in scala di grigi se necessario, e la ridimensiona.\n",
        "    \"\"\"\n",
        "    image = imread(path)\n",
        "    if image.shape[-1] == 4:\n",
        "        image = rgba2rgb(image)\n",
        "    if len(image.shape) > 2:\n",
        "        image = rgb2gray(image)\n",
        "    image_resized = resize(image, (512, 512), anti_aliasing=True)\n",
        "    image_bin = image_resized > 0.5  # Soglia per binarizzazione\n",
        "    return image_bin\n",
        "\n",
        "for phase in ['train', 'val', 'test']:\n",
        "    gt_masks_folder = gt_masks_folders[phase]\n",
        "    reconstructed_masks_folder = reconstructed_masks_folders[phase]\n",
        "\n",
        "    reconstructed_filenames_set = {os.path.splitext(fname.replace('_patch', ''))[0] for fname in os.listdir(reconstructed_masks_folder) if fname.endswith('.png')}\n",
        "\n",
        "    for fname in os.listdir(gt_masks_folder):\n",
        "        if not fname.endswith('.png') or fname == \"448_G.png\":\n",
        "            continue\n",
        "\n",
        "        base_fname = os.path.splitext(fname)[0]\n",
        "\n",
        "        if base_fname in reconstructed_filenames_set:\n",
        "            ground_truth_path = os.path.join(gt_masks_folder, fname)\n",
        "            reconstructed_mask_path = os.path.join(reconstructed_masks_folder, base_fname + '.png')\n",
        "\n",
        "            mask_gt = prepare_image(ground_truth_path)\n",
        "            reconstructed_mask = prepare_image(reconstructed_mask_path)\n",
        "\n",
        "            iou, accuracy, precision, sensitivity, dice = calculate_metrics(mask_gt, reconstructed_mask)\n",
        "\n",
        "            # Aggiungi i risultati sia alle metriche aggregate\n",
        "            aggregated_metrics['iou'].append(iou)\n",
        "            aggregated_metrics['accuracy'].append(accuracy)\n",
        "            aggregated_metrics['precision'].append(precision)\n",
        "            aggregated_metrics['sensitivity'].append(sensitivity)\n",
        "            aggregated_metrics['dice'].append(dice)\n",
        "\n",
        "def print_aggregated_metrics(metrics):\n",
        "    print(\"Metrics for ALL PHASES combined:\")\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        if metric_values:  # Verifica che la lista non sia vuota\n",
        "            mean_value = np.mean(metric_values)\n",
        "            std_value = np.std(metric_values)\n",
        "            print(f\"{metric_name.capitalize()}: mean = {mean_value:.4f} +/- {std_value:.4f}\")\n",
        "        else:\n",
        "            print(f\"{metric_name.capitalize()}: No data\")\n",
        "    print()  # Aggiunge una riga vuota per separare le sezioni\n",
        "\n",
        "# Stampa le metriche aggregate\n",
        "print_aggregated_metrics(aggregated_metrics)\n"
      ],
      "metadata": {
        "id": "0qxFjw2Sk8qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Assumendo che i file siano stati caricati e che le loro directory siano state impostate correttamente\n",
        "original_images_dir = '/content/drive/MyDrive/Progetto/RETINA/test/test_x'\n",
        "ground_truth_dir = '/content/drive/MyDrive/Progetto/RETINA/test/test_y'\n",
        "reconstructed_masks_dir = '/content/drive/MyDrive/Progetto/RETINA/reconstructed512/test'\n",
        "\n",
        "# Prendi i primi quattro file di ogni directory (assumendo che i file siano ordinati correttamente)\n",
        "original_images_files = sorted(os.listdir(original_images_dir))[:4]\n",
        "ground_truth_files = sorted(os.listdir(ground_truth_dir))[:4]\n",
        "reconstructed_masks_files = sorted(os.listdir(reconstructed_masks_dir))[:4]\n",
        "\n",
        "# Crea una figura con subplot per le 12 immagini (3 per ogni categoria)\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for i in range(4):\n",
        "    # Immagine originale RGB\n",
        "    plt.subplot(3, 4, i + 1)\n",
        "    original_image = Image.open(os.path.join(original_images_dir, original_images_files[i]))\n",
        "    plt.imshow(original_image)\n",
        "    plt.title(f'Original {i+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Ground truth\n",
        "    plt.subplot(3, 4, i + 5)\n",
        "    ground_truth_image = Image.open(os.path.join(ground_truth_dir, ground_truth_files[i]))\n",
        "    plt.imshow(ground_truth_image, cmap='gray')\n",
        "    plt.title(f'Ground Truth {i+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Maschera ricostruita\n",
        "    plt.subplot(3, 4, i + 9)\n",
        "    reconstructed_mask = Image.open(os.path.join(reconstructed_masks_dir, reconstructed_masks_files[i]))\n",
        "    plt.imshow(reconstructed_mask, cmap='gray')\n",
        "    plt.title(f'Reconstructed Mask {i+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "# Mostra il plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wq1sMANVqwsk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
